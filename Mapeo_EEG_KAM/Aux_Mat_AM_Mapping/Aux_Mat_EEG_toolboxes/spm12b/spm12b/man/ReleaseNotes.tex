\documentclass[a4paper,titlepage,openany]{article}
\usepackage{epsfig,amsmath,pifont,moreverb,multirow,multicol}

%\usepackage[scaled=.92]{helvet}
%\usepackage{newcent}
%\usepackage{bookman}
%\usepackage{utopia}
%\usepackage{avant}
%\usepackage{charter}
%\usepackage{mathpazo}
\renewcommand{\familydefault}{\sfdefault}

\usepackage[colorlinks=true,
pdfpagemode=UseOutlines,
pdftitle={SPM12 Release Notes},
pdfauthor={The SPM Developers},
pdfsubject={Statistical Parametric Mapping},
pdfkeywords={neuroimaging, MRI, PET, EEG, MEG, SPM}
]{hyperref}
\pagestyle{headings}
\bibliographystyle{plain}

\hoffset=15mm
\voffset=-5mm
\oddsidemargin=0mm
\evensidemargin=0mm
\topmargin=0mm
\headheight=12pt
\headsep=10mm
\textheight=240mm
\textwidth=148mm
\marginparsep=5mm
\marginparwidth=21mm
\footskip=10mm
\parindent 0pt
\parskip 6pt

\newcommand{\matlab}{\textsc{MATLAB}}

\begin{document}

\let\oldlabel=\label
\renewcommand{\label}[1]{
{\pdfdest name {#1} fit}
\oldlabel{#1}
}

\newlength{\centeroffset}
\setlength{\centeroffset}{-0.5\oddsidemargin}
\addtolength{\centeroffset}{0.5\evensidemargin}
%\addtolength{\textwidth}{-\centeroffset}
\thispagestyle{empty}
\vspace*{\stretch{1}}
\noindent\hspace*{\centeroffset}\makebox[0pt][l]{
\begin{minipage}{\textwidth}
\flushright
\textbf{\Huge{SPM12b Release Notes}}
{\noindent\rule[-1ex]{\textwidth}{5pt}\\[2.5ex]}
\hfill{{\huge The FIL Methods Group} \\ {\LARGE (and honorary members)}\\}
%\vspace{20mm}
\end{minipage}
}

%\vspace{\stretch{2}}
\noindent\hspace*{\centeroffset}\makebox[0pt][l]{
\begin{minipage}{\textwidth}
\flushright
{\footnotesize
Functional Imaging Laboratory\\
Wellcome Trust Centre for Neuroimaging\\
Institute of Neurology, UCL\\
12 Queen Square, London WC1N 3BG, UK\\
\today\\
\url{http://www.fil.ion.ucl.ac.uk/spm/}}
\end{minipage}}

\vspace{10mm}

%\section*{Welcome to SPM12b}
This is SPM12b, the beta version of the next major release of SPM\footnote{\url{http://www.fil.ion.ucl.ac.uk/spm/software/spm12/}}.
Public updates are taking place frequently until the final release (see spm\_update.m).
We are always interested to hear feedbacks and comments from SPM users - please contact us at \href{mailto:fil.spm@ucl.ac.uk}{fil.spm@ucl.ac.uk}.
If you happen to find any bug, please report them at the same email address. Thank you!


SPM is free but copyright software, distributed under the terms of the GNU General Public Licence as published by the Free Software Foundation (either version 2, as given in file LICENCE.txt, or at your option, any later version)\footnote{\url{http://www.gnu.org/copyleft/}}. \\
SPM is developed under the auspices of Functional Imaging Laboratory (FIL), The Wellcome Trust Centre for NeuroImaging, in the Institute of Neurology at University College London (UCL), UK.

%Copyright (C) 1991,1994-2013 Wellcome Trust Centre for Neuroimaging.

\vspace{10mm}
\begin{multicols}{2}

\section{Temporal processing}

\section{Spatial processing}
There have been changes to much of the functionality for spatially transforming images -- particularly with respect to inter-subject registration.
This is a small step towards reducing SPM to a more manageable size \cite{ashburner2011spm}.

\subsection{Normalise}
Spatial normalisation is no longer based on minimising the mean squared difference between a template and a warped version of the image.
Instead, it is now done via segmentation \cite{ashburner05}, as this provides more flexibility.
For those of you who preferred the older way of spatially normalising images, this is still available via the ``Old Normalise'' Tool.
However, the aim is to try to simplify SPM and eventually remove the older and less effective \cite{klein_evaluation} routines.

Deformation fields are now saved in a form that allows much more precise alignment.
Rather than the old sn.mat format, they are now saved as y\_$*$.nii files, which contain three image volumes encoding the x, y and z coordinates (in mm) of where each voxel maps to.

Note that for spatially normalising PET, SPECT and other images that have spatially correlated noise, it is a good idea to change the smoothness setting on the user interface (from 0 to about 5 mm).

\subsection{Segment}
The default segmentation has now been replaced by a slightly modified version of what was unimaginatively called ``New Segment'' in SPM8.
For those of you who preferred the older way of segmenting images, this is still available via the ``Old Segment'' Tool.
The aim, however, is to try to simplify SPM and eventually remove the older functionality that works less well.
Both implementations are based on the algorithm presented in \cite{ashburner05}, although the newer version makes use of additional tissue classes, allows multi-channel segmentation (of eg T2-weighted and PD-weighted images), and incorporates a more flexible image registration component.

Changes to the SPM8 version of ``New Segment'' include different regularisation for the deformations, some different default settings, as well as re-introducing the re-scaling of the tissue probability maps (which was in the old segment, but not the new).  In addition, the tissue probability maps were re-generated using the T2-weighted and PD-weighted scans from the IXI dataset\footnote{\url{http://www.brain-development.org/}}.
This was initially done in an automated way (by enabling a hidden feature in spm\_preproc\_run.m, which allows the necessary sufficient statistics for re-generating the templates to be computed), with some manual editing of the results to tidy them up.
Note that eyeballs are now included within the same class as CSF.
Separating eyeballs from other non-brain tissue allows the nonlinear registration part to be made more flexible, but the disadvantage is that intra-cranial volumes are now fractionally more difficult to compute.
However, the cleanup step (re-introduced from the old segmentation routine, and extended slightly) should allow eyeballs to be removed from the fluid tissue class.

\section{fMRI Statistics}

\subsection{Canonical Variates Analysis}

SPM's Canonical Variates Analysis (CVA) function, \verb!spm_cva.m!, has been updated so that it also computes Log Bayes Factors to make inferences about the number of significantly non-zero canonical vectors. It computes AIC and BIC approximations to these Log Bayes Factors. These quantities can be used in second-level analysis to make inferences about a group of subjects/sessions using the SPM's random effects model selection tool (\verb!spm_BMS.m!). 

\subsection{Regional Bayesian model comparison - first level}

The function \verb!spm_vb_regionF.m! now provides approximation of the log model evidence for a first level fMRI model. If you call this function multiple times with different design matrices it will allow you to implement Bayesian model comparison. This is useful for example for comparing non-nested GLMs with, for example, different (sets of) parametric modulators. The approach can be applied to first level fMRI data from a local region. This is the recommended method for Bayesian comparison of General Linear Models of first level fMRI data.

\subsection{Interactive Bayesian model comparison - second level}

Multidimensional (or two-sided) inferences about GLM parameters 
 are now implemented using Savage-Dickey ratios.  When you specify a multidimensional contrast for a GLM that has been estimated using the Bayesian option, SPM will produce log Bayes factor maps (in favour of the alternative versus the null model). This provides a Bayesian analogue of the F-test and allows one to implement Bayesian model comparisons in a truly interactive manner.
This is the recommended method for Bayesian model comparison of second level data (fMRI or MEG/EEG) .

\subsection{DCM}

The bilinear model used by (single-state) DCM for fMRI has been upgraded. To summarise, a number of people have observed that Bayesian parameter averages (BPA) of self (intrinsic) connections can be positive, despite being negative for each subject or session. This is perfectly plausible behaviour -- in the sense that their prior expectation is $-1/2$ and each subject can provide evidence that the posterior is less than the prior mean (but still negative). When this evidence is accumulated by BPA, the posterior can be `pushed' into positive values. The anomalous aspect of this behaviour rests on the fact that there is no negativity constraint on the self-connections.  In the revised code, the self-connections -- in the leading diagonal of the $A$ matrix -- now specify log scaling parameters. This means that these (and only these) parameters encode a self-connections of $-1/2*exp(A)$; where $A$ has a prior mean of $0$ and $-1/2*exp(0) = -1/2$. 

This re-parameterisation does not affect model inversion very much but guarantees the BPA is always negative because $-1/2*exp(A) < 0$ has to be less than one. The parameterisation of connection strengths in terms of log scale parameters is already used by two-state models for fMRI and all EEG (and MEG) DCMs.

\section{EEG/MEG}

\subsection{@meeg object}

The following @meeg methods have been removed to simplify and rationalise the object interface
\begin{itemize}
\item   'pickconditions' - replaced with 'indtrial'
\item   'meegchannels', 'ecgchannels', 'emgchannels',  'eogchannels' - replaced with 'indchantype'
\item 'reject' - replaced with 'badtrials'
\end{itemize}

It is now possible to have @meeg objects without a linked data file. This was useful to simplify the conversion code. Also in 'Prepare' one can just load a header of a raw data file and use it to prepare inputs for the full conversion batch (e.g. select channels or prepare trial definition for epoching during conversion).


\subsection{Preprocessing}

The preprocessing functions now use the SPM batch system and the interactive GUI elements have been removed. This should make it easy to build processing pipelines for performing complete complicated data analyses without programming. The use of batch has many advantages but can also complicate some of the operations because a batch must be configured in advance and cannot rely on information available in the input file. For instance, the batch tool cannot know the channel names for a particular dataset and thus cannot generate a dialog box for the user to choose the channels. To facilitate the processing steps requiring this kind of information additional functionalities have been added to the 'Prepare' tool under 'Batch inputs' menu. One can now make the necessary choices for a particular dataset using an unteractive GUI and then save the results in a mat file and use this file as an input to batch.

It is still also possible to call the preprocessing function via scripts and scripts can be generated using History2script functionality as in SPM8. The inputs structures and parameters names of preprocessing function have been standardised so that e.g. time window is always called S.timewin and units of peristimulus time are always ms. Also substructures of S have been removed except for where they are functionally essential.

Simple GUI functionality for converting a variable in the workspace into SPM M/EEG dataset was added to 'Prepare' (File/Import from workspace).

For MEG system with planar gradiometers (e.g. Neuromag) there is a new tool for combining planar channels and writing out the results back into an M/EEG file. This allows further processing of combined channels which was impossible in SPM8 e.g. baseline correction or TF rescaling.

Conversion to images was completely rewritten. It is now possible to easily create 1D images and average across any set of dimensions to reduce the number of multiple comparisons.

\subsection{Convolution modelling}
An implementation of recently published (Litvak et al. 2012) convolution modelling method for M/EEG power has been added (under 'Specify 1st-level') button. It is now possible to apply TF analysis to continuous data and save continuous TF datasets to be used as input to this tool.


\subsection{DCM}

\begin{itemize}
\item The routine evaluating cross spectra (\texttt{spm\_dcm\_data\_csd}) now performs a moving window cross spectral analysis (based on an eighth order MAR model) to remove (nonstationary) fluctuations in the cross spectra. This is achieved by performing a singular value decomposition on the time-dependent cross spectra and retaining only the principal spectral mode.
\item The data features used for inverting dynamic causal models of cross spectral density now include both the cross spectra per se and the cross covariance functions. These are simply concatenated to provide a greater latitude of data features to compute free energy gradients. Heuristically, the cross spectra inform gradients that affect low frequencies, while the covariance functions allow high frequencies to be fitted gracefully. This means that any frequency dependent precision can be removed.
\item The inversion routines for event related potentials (\texttt{spm\_dcm\_erp}) and complex cross spectra (\texttt{spm\_dcm\_csd}) now use more precise (hyper) priors on data feature noise (with an expected log precision \texttt{hE} of eight and a log precision of eight). Effectively, this means that, a priori, we expect these data features to be essentially noiseless -- because they accumulate information from long timeseries, with many degrees of freedom.
\item To ensure the quantitative veracity of the hyperpriors, the data are scaled to have a maximum amplitude of one (for evoked responses) and a variance of 16 (four cross spectral density analysis).The scaling of the exogenous and endogenous input (\texttt{U}) in the equations of motion has also been adjusted, to ensure that the neural mass and mean field models used in DCM produce an ERP with a maximum height of about 1 and autospectra with about unit variance.
\item The amplitude of evoked responses, and the spectral responses (shown in terms of autospectra and covariance functions) can now be visualised -- for all models -- using the survey of models button in the \texttt{Neural\_models} demo.
\end{itemize}

\section{Utilities}

\subsection{DICOM Import}
The DICOM dictionary has been updated to reflect changes to the standard over the last decade or so.
It is now based on the 2011 edition\footnote{\url{http://medical.nema.org/standard.html}}.

\subsection{Deformations}
The deformations utility was completely re-written to provide additional flexibility.
This was largely to facilitate the re-write of what lies behind the ``Normalise'' button.

\section{Tools}

\subsection{Dartel}
Much of C the code (in the mex functions that do most of the work in Dartel) has been extensively re-written to make it work more effectively, and to provide a framework on which to base the ``Shoot'' and ``Longitudinal Registration'' toolboxes.

\subsection{Shoot}
This toolbox is based on the work in \cite{ashburner2011diffeomorphic}, and is a diffeomorphic registration approach similar to Dartel, although much more theoretically sound.
Evaluations show that it achieves more robust solutions in situations where larger deformations are required.
The eventual plan will be to replace Dartel with this toolbox, although more work needs to be done in terms of user interfaces etc.

\subsection{Longitudinal Registration}
SPM12 incorporates a new longitudinal registration approach \cite{ashburner2013symmetric}, which replaces the old ``high-dimensional warping'' toolbox.
It essentially now involves a group-wise intra-subject modeling framework, which combines diffeomorphic \cite{ashburner2011diffeomorphic} and rigid-body registration, incorporating a correction for the intensity inhomogeneity artifact usually seen in MRI data.
Recently, systematic bias in longitudinal image registration has become a more notable issue.
The aim of this toolbox is to estimate volume changes that do not depend on whether the first time point is aligned to the second, or vice verca.

\subsection{Old Segment}
The default segmentation approach in SPM12 is now based on what was known as ``New Segment'' in SPM8.
This toolbox keeps the old segmentation approach available to those who wish to continue using it, although we plan to eventually phase out the older approach.

\subsection{Old Normalise}
The default spatial normalisation approach in SPM12 is now based on what was known as ``New Segment'' in SPM8.
This toolbox keeps the old normalisation approach available to those who wish to continue using it, although we plan to eventually phase out the older approach.
See \cite{klein_evaluation} to see how poorly the old normalisation approach works.
It was definitely time for it to go.

\section{Batch Interface}

%\section{Bibliography}

\bibliography{biblio/methods_macros,biblio/methods_group,biblio/external}

\end{multicols}
\end{document}

